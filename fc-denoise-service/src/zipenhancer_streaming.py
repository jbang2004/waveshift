#!/usr/bin/env python3
"""
ZipEnhancer æµå¼éŸ³é¢‘é™å™ªå·¥å…· - æ”¯æŒç¯å¢ƒå˜é‡é…ç½®
æ¼”ç¤ºå¦‚ä½•å°†éŸ³é¢‘åˆ†å—å¤„ç†ä»¥èŠ‚çœå†…å­˜ï¼Œæ”¯æŒåŠ¨æ€çº¿ç¨‹é…ç½®
"""

import soundfile as sf
import numpy as np
import torch
import onnxruntime
import os
from typing import Generator, Tuple

def mag_pha_stft(signal, n_fft=400, hop_size=100, win_size=400, compress_factor=0.3):
    """STFTå˜æ¢"""
    window = torch.hann_window(win_size)
    stft = torch.stft(signal, n_fft, hop_size, win_size, window, center=True, 
                      pad_mode='reflect', return_complex=True)
    magnitude = torch.pow(torch.abs(stft), compress_factor)
    phase = torch.angle(stft)
    return magnitude, phase

def mag_pha_istft(magnitude_compressed, phase, n_fft=400, hop_size=100, win_size=400, compress_factor=0.3):
    """ISTFTå˜æ¢"""
    magnitude = torch.pow(magnitude_compressed, 1.0 / compress_factor)
    real = magnitude * torch.cos(phase)
    imag = magnitude * torch.sin(phase)
    stft_complex = torch.complex(real, imag)
    window = torch.hann_window(win_size)
    signal = torch.istft(stft_complex, n_fft, hop_size, win_size, window, center=True)
    return signal

def get_onnx_thread_config():
    """
    ä»ç¯å¢ƒå˜é‡è·å–ONNX Runtimeçº¿ç¨‹é…ç½®
    æ”¯æŒå¤šç§ç¯å¢ƒå˜é‡ï¼Œæä¾›åˆç†çš„é»˜è®¤å€¼å’ŒéªŒè¯
    """
    
    # 1. ä¼˜å…ˆä½¿ç”¨ä¸“ç”¨çš„ONNX Runtimeç¯å¢ƒå˜é‡
    intra_threads = os.getenv('ORT_INTRA_OP_NUM_THREADS')
    inter_threads = os.getenv('ORT_INTER_OP_NUM_THREADS')
    
    # 2. å¦‚æœæ²¡æœ‰ä¸“ç”¨é…ç½®ï¼Œä½¿ç”¨é€šç”¨çš„ORT_NUM_THREADS  
    if not intra_threads or not inter_threads:
        ort_threads = os.getenv('ORT_NUM_THREADS')
        if ort_threads:
            thread_count = int(ort_threads)
            intra_threads = intra_threads or str(thread_count)
            inter_threads = inter_threads or str(thread_count)
    
    # 3. æœ€åfallbackåˆ°ç³»ç»Ÿçº§çº¿ç¨‹é…ç½®
    if not intra_threads:
        intra_threads = os.getenv('OMP_NUM_THREADS', '2')
    if not inter_threads:
        inter_threads = os.getenv('OMP_NUM_THREADS', '2')
    
    # è½¬æ¢ä¸ºæ•´æ•°å¹¶éªŒè¯èŒƒå›´
    try:
        intra_count = max(1, min(8, int(intra_threads)))  # é™åˆ¶åœ¨1-8èŒƒå›´
        inter_count = max(1, min(8, int(inter_threads)))  # é™åˆ¶åœ¨1-8èŒƒå›´
    except ValueError:
        print(f"âš ï¸ æ— æ•ˆçš„çº¿ç¨‹é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼: intra=2, inter=2")
        intra_count, inter_count = 2, 2
    
    return intra_count, inter_count

def get_onnx_optimization_level():
    """ä»ç¯å¢ƒå˜é‡è·å–å›¾ä¼˜åŒ–çº§åˆ«"""
    level_str = os.getenv('ORT_GRAPH_OPTIMIZATION_LEVEL', 'all').lower()
    
    level_map = {
        'disable': onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL,
        'basic': onnxruntime.GraphOptimizationLevel.ORT_ENABLE_BASIC,
        'extended': onnxruntime.GraphOptimizationLevel.ORT_ENABLE_EXTENDED,
        'all': onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
    }
    
    return level_map.get(level_str, onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL)

class StreamingZipEnhancer:
    """æµå¼ZipEnhancerå¤„ç†å™¨"""
    
    def __init__(self, onnx_model_path: str, chunk_size: int = 16000, overlap_size: int = 1600, 
                 chunk_duration: float = 1.0, overlap_duration: float = 0.5):
        """
        åˆå§‹åŒ–æµå¼å¤„ç†å™¨
        
        Args:
            onnx_model_path: ONNXæ¨¡å‹è·¯å¾„
            chunk_size: æ¯ä¸ªå¤„ç†å—çš„å¤§å°ï¼ˆæ ·æœ¬æ•°ï¼‰
            overlap_size: é‡å åŒºåŸŸå¤§å°ï¼ˆç”¨äºæ¶ˆé™¤è¾¹ç•Œæ•ˆåº”ï¼‰
            chunk_duration: æ¯ä¸ªå¤„ç†å—çš„æ—¶é•¿ï¼ˆç§’ï¼‰
            overlap_duration: å—ä¹‹é—´çš„é‡å æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        # ğŸš€ æ”¯æŒç¯å¢ƒå˜é‡çš„ONNX Runtimeé…ç½® - è‡ªåŠ¨é€‚é…vCPUæ•°é‡
        intra_threads, inter_threads = get_onnx_thread_config()
        optimization_level = get_onnx_optimization_level()
        
        sess_options = onnxruntime.SessionOptions()
        sess_options.intra_op_num_threads = intra_threads
        sess_options.inter_op_num_threads = inter_threads
        sess_options.graph_optimization_level = optimization_level
        
        # æ‰“å°é…ç½®ç”¨äºè°ƒè¯•
        cpu_config = os.getenv('CPU_CONFIG', 'unknown')
        print(f"ğŸ¯ ONNX Runtimeé…ç½® ({cpu_config}): intra={intra_threads}, inter={inter_threads}, optimization={optimization_level.name}")
        
        # ä»ç¯å¢ƒå˜é‡è·å–é¢å¤–çš„æ€§èƒ½é…ç½®
        if os.getenv('ORT_ENABLE_CPU_FP16_OPS', '0') == '1':
            print("ğŸ”§ å¯ç”¨CPU FP16æ“ä½œä¼˜åŒ–")
        
        self.session = onnxruntime.InferenceSession(
            onnx_model_path, 
            sess_options=sess_options
        )
        self.chunk_size = chunk_size
        self.overlap_size = overlap_size
        self.previous_chunk = None
        
        # æ”¯æŒæ—¶é—´å‚æ•°
        self.sample_rate = 16000
        self.chunk_duration = chunk_duration
        self.overlap_duration = overlap_duration
        
        # STFTå‚æ•°
        self.n_fft = 400
        self.hop_size = 100
        self.win_size = 400
        self.compress_factor = 0.3
        
    def process_chunk(self, chunk: np.ndarray) -> np.ndarray:
        """å¤„ç†å•ä¸ªéŸ³é¢‘å—"""
        # è½¬æ¢ä¸ºtorchå¼ é‡
        chunk_tensor = torch.from_numpy(chunk.astype(np.float32)).unsqueeze(0)
        
        # RMSå½’ä¸€åŒ–
        rms = torch.sqrt(torch.mean(chunk_tensor ** 2))
        norm_factor = 1.0 / (rms + 1e-10)
        normalized_chunk = chunk_tensor * norm_factor
        
        # STFT
        chunk_amp, chunk_pha = mag_pha_stft(normalized_chunk)
        
        # ONNXæ¨ç†
        ort_inputs = {
            self.session.get_inputs()[0].name: chunk_amp.numpy(),
            self.session.get_inputs()[1].name: chunk_pha.numpy()
        }
        ort_outs = self.session.run(None, ort_inputs)
        
        # ISTFTé‡æ„
        amp_g = torch.from_numpy(ort_outs[0])
        pha_g = torch.from_numpy(ort_outs[1])
        enhanced_chunk = mag_pha_istft(amp_g, pha_g)
        
        # åå½’ä¸€åŒ–
        enhanced_chunk = enhanced_chunk / norm_factor
        
        return enhanced_chunk.numpy()[0]
    
    def stream_process(self, audio: np.ndarray) -> Generator[np.ndarray, None, None]:
        """
        æµå¼å¤„ç†éŸ³é¢‘
        
        Args:
            audio: è¾“å…¥éŸ³é¢‘æ•°ç»„
            
        Yields:
            å¤„ç†åçš„éŸ³é¢‘å—
        """
        total_samples = len(audio)
        processed_samples = 0
        
        while processed_samples < total_samples:
            # è®¡ç®—å½“å‰å—çš„èµ·å§‹å’Œç»“æŸä½ç½®
            start = processed_samples
            end = min(start + self.chunk_size, total_samples)
            
            # æå–å½“å‰å—
            current_chunk = audio[start:end]
            
            # å¦‚æœä¸æ˜¯ç¬¬ä¸€å—ï¼Œæ·»åŠ é‡å åŒºåŸŸ
            if self.previous_chunk is not None:
                # ä»å‰ä¸€å—å–é‡å éƒ¨åˆ†
                overlap_part = self.previous_chunk[-self.overlap_size:]
                current_chunk = np.concatenate([overlap_part, current_chunk])
            
            # å¤„ç†å½“å‰å—
            enhanced_chunk = self.process_chunk(current_chunk)
            
            # å¦‚æœæœ‰é‡å ï¼Œè·³è¿‡é‡å éƒ¨åˆ†
            if self.previous_chunk is not None:
                enhanced_chunk = enhanced_chunk[self.overlap_size:]
            
            # ä¿å­˜å½“å‰å—ä¾›ä¸‹æ¬¡ä½¿ç”¨
            self.previous_chunk = current_chunk
            
            # è¾“å‡ºå¤„ç†ç»“æœ
            yield enhanced_chunk
            
            processed_samples = end
    
    def process(self, audio: np.ndarray) -> np.ndarray:
        """
        ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªéŸ³é¢‘ï¼ˆé€‚åˆçŸ­éŸ³é¢‘ï¼‰
        
        Args:
            audio: è¾“å…¥éŸ³é¢‘æ•°ç»„
            
        Returns:
            é™å™ªåçš„éŸ³é¢‘æ•°ç»„
        """
        # å¯¹äºçŸ­éŸ³é¢‘ï¼Œç›´æ¥å¤„ç†æ•´ä¸ªéŸ³é¢‘
        if len(audio) <= self.chunk_size * 2:
            return self.process_chunk(audio)
        
        # å¯¹äºé•¿éŸ³é¢‘ï¼Œä½¿ç”¨æµå¼å¤„ç†å¹¶åˆå¹¶ç»“æœ
        enhanced_chunks = []
        for chunk in self.stream_process(audio):
            enhanced_chunks.append(chunk)
        
        return np.concatenate(enhanced_chunks)

def streaming_denoise(input_path: str, output_path: str, 
                     onnx_model_path: str = './speech_zipenhancer_ans_multiloss_16k_base/onnx_model.onnx',
                     chunk_duration: float = 1.0):
    """
    æµå¼éŸ³é¢‘é™å™ª
    
    Args:
        input_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
        output_path: è¾“å‡ºéŸ³é¢‘è·¯å¾„  
        onnx_model_path: ONNXæ¨¡å‹è·¯å¾„
        chunk_duration: æ¯å—å¤„ç†æ—¶é•¿ï¼ˆç§’ï¼‰
    """
    print(f"ğŸŒŠ å¼€å§‹æµå¼é™å™ªå¤„ç†: {input_path}")
    
    # è¯»å–éŸ³é¢‘ - ä½¿ç”¨soundfileæ›¿ä»£librosaä»¥å‡å°é•œåƒå¤§å°
    audio, sr = sf.read(input_path)
    # è½¬æ¢ä¸º16kHzå•å£°é“
    if len(audio.shape) > 1:
        audio = np.mean(audio, axis=1)  # è½¬ä¸ºå•å£°é“
    if sr != 16000:
        # ç®€å•é‡é‡‡æ ·ï¼ˆä½¿ç”¨numpyçº¿æ€§æ’å€¼ï¼Œé¿å…scipyä¾èµ–ï¼‰
        original_length = len(audio)
        target_length = int(original_length * 16000 / sr)
        audio = np.interp(
            np.linspace(0, original_length - 1, target_length),
            np.arange(original_length),
            audio
        )
        sr = 16000
    
    # è®¡ç®—å—å¤§å°
    chunk_size = int(chunk_duration * sr)
    overlap_size = chunk_size // 10  # 10%é‡å 
    
    print(f"ğŸ“Š éŸ³é¢‘ä¿¡æ¯: {sr}Hz, {len(audio)/sr:.2f}ç§’")
    print(f"ğŸ“Š å¤„ç†é…ç½®: å—å¤§å°={chunk_size}æ ·æœ¬, é‡å ={overlap_size}æ ·æœ¬")
    
    # åˆ›å»ºæµå¼å¤„ç†å™¨
    enhancer = StreamingZipEnhancer(onnx_model_path, chunk_size, overlap_size)
    
    # æ”¶é›†æ‰€æœ‰å¤„ç†åçš„å—
    enhanced_chunks = []
    
    for enhanced_chunk in enhancer.stream_process(audio):
        enhanced_chunks.append(enhanced_chunk)
    
    # åˆå¹¶æ‰€æœ‰å—
    enhanced_audio = np.concatenate(enhanced_chunks)
    
    # ç¡®ä¿é•¿åº¦åŒ¹é…ï¼ˆå¯èƒ½å› ä¸ºé‡å å¤„ç†æœ‰å°å¹…å·®å¼‚ï¼‰
    if len(enhanced_audio) > len(audio):
        enhanced_audio = enhanced_audio[:len(audio)]
    elif len(enhanced_audio) < len(audio):
        # ç”¨é›¶å¡«å……
        padding = np.zeros(len(audio) - len(enhanced_audio))
        enhanced_audio = np.concatenate([enhanced_audio, padding])
    
    # ä¿å­˜ç»“æœ
    enhanced_audio = np.clip(enhanced_audio, -1.0, 1.0)
    sf.write(output_path, (enhanced_audio * 32767).astype(np.int16), sr)
    
    print(f"âœ… æµå¼é™å™ªå®Œæˆ! è¾“å‡º: {output_path}")
    print(f"ğŸ“ˆ å†…å­˜å³°å€¼çº¦ä¸º: {chunk_size * 4 / 1024 / 1024:.1f}MB (vs æ‰¹å¤„ç†: {len(audio) * 4 / 1024 / 1024:.1f}MB)")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        input_file = "./speech_zipenhancer_ans_multiloss_16k_base/examples/noisy_sample.wav"
        output_file = "streaming_enhanced.wav"
    else:
        input_file = sys.argv[1]
        output_file = sys.argv[2] if len(sys.argv) > 2 else "streaming_enhanced.wav"
    
    streaming_denoise(input_file, output_file) 