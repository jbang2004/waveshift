#!/usr/bin/env python3
"""
FCé™å™ªæœåŠ¡å™¨ - é˜¿é‡Œäº‘å‡½æ•°è®¡ç®—é€‚é…ç‰ˆ (ç®€åŒ–ç‰ˆ)
åŸºäºæˆåŠŸçš„Alpineä¼˜åŒ–é™å™ªæœåŠ¡å™¨ï¼Œé€‚é…é˜¿é‡Œäº‘FCç¯å¢ƒ

ğŸš€ FC 3.0ä¼˜åŒ–ç‰¹æ€§:
- æ‡’åŠ è½½æ¨¡å¼: é¿å…å¯åŠ¨è¶…æ—¶é—®é¢˜
- åŠ¨æ€çº¿ç¨‹é…ç½®: é€‚é…4vCPUç¯å¢ƒå˜é‡
- å¢å¼ºå†…å­˜ç®¡ç†: é€‚é…AIæ¨¡å‹çš„å†…å­˜æ¸…ç†
- FCç”Ÿå‘½å‘¨æœŸç®¡ç†: PreStopå›è°ƒèµ„æºæ¸…ç†
- æ€§èƒ½ç›‘æ§: å†·çƒ­å¯åŠ¨ã€æ¨¡å‹å¤ç”¨ç»Ÿè®¡

ğŸ”§ å¯é…ç½®ç¯å¢ƒå˜é‡:
- ORT_INTRA_OP_NUM_THREADS=3: ONNXå†…éƒ¨å¹¶è¡Œçº¿ç¨‹æ•°
- ORT_INTER_OP_NUM_THREADS=2: ONNXæ“ä½œé—´å¹¶è¡Œçº¿ç¨‹æ•°

ğŸ“Š æ€§èƒ½é¢„æœŸ:
- é¦–æ¬¡è¯·æ±‚: ~15ç§’ (æ‡’åŠ è½½æ¨¡å¼)
- åç»­è¯·æ±‚: 7-10ç§’ (æ¨¡å‹å¤ç”¨)
- å®ä¾‹å¤ç”¨ç‡: >90% (åœ¨audio-segmentæµå¼åœºæ™¯ä¸­)
"""

from fastapi import FastAPI, Response, Header, HTTPException, Request
from fastapi.responses import JSONResponse
import io
import numpy as np
import soundfile as sf
import time
import logging
import sys
import os
import gc
import base64
from typing import Optional

# å»¶è¿Ÿå¯¼å…¥torchå’Œonnxruntimeï¼Œé¿å…å¯åŠ¨æ—¶å´©æºƒ
torch = None
onnxruntime = None
StreamingZipEnhancer = None

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FCæ€§èƒ½ç›‘æ§æŒ‡æ ‡ (ç®€åŒ–ç‰ˆ)
fc_metrics = {
    'instance_start_time': time.time(),
    'cold_starts': 0,
    'warm_starts': 0,
    'model_reuse_count': 0,
    'total_requests': 0
}

print(f"[FC-STARTUP] Loading FC denoise server...")
print(f"[FC-STARTUP] Python {sys.version}")

# åˆ›å»ºFastAPIåº”ç”¨ - FCé€‚é…
app = FastAPI(
    title="FC ZipEnhancer Denoise Service", 
    version="1.0.0",
    description="é˜¿é‡Œäº‘FCé™å™ªæœåŠ¡ - åŸºäºZipEnhancerçš„éŸ³é¢‘é™å™ª"
)

# å…¨å±€é™å™ªå™¨å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
global_enhancer = None
model_loading = False
dependencies_loaded = False

def load_dependencies():
    """æ‡’åŠ è½½Pythonä¾èµ–"""
    global torch, onnxruntime, StreamingZipEnhancer, dependencies_loaded
    
    if dependencies_loaded:
        return True
    
    try:
        logger.info("ğŸ”„ åŠ è½½æ·±åº¦å­¦ä¹ ä¾èµ–...")
        import torch as torch_module
        torch = torch_module
        # âœ… åŠ¨æ€é€‚é…ç¯å¢ƒå˜é‡ï¼Œæ¶ˆé™¤çº¿ç¨‹é…ç½®å†²çª
        thread_count = int(os.getenv('ORT_INTRA_OP_NUM_THREADS', '3'))
        torch.set_num_threads(thread_count)
        logger.info(f"ğŸ”§ Torchçº¿ç¨‹é…ç½®: {thread_count} (åŠ¨æ€é€‚é…ç¯å¢ƒå˜é‡)")
        
        import onnxruntime as ort_module
        onnxruntime = ort_module
        
        from zipenhancer_streaming import StreamingZipEnhancer as enhancer_class
        StreamingZipEnhancer = enhancer_class
        
        dependencies_loaded = True
        logger.info("âœ… ä¾èµ–åŠ è½½æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"âŒ ä¾èµ–åŠ è½½å¤±è´¥: {e}")
        return False

def get_enhancer():
    """æ‡’åŠ è½½é™å™ªå™¨å®ä¾‹"""
    global global_enhancer, model_loading
    
    if global_enhancer is not None:
        return global_enhancer
    
    if model_loading:
        return None
    
    # ç¡®ä¿ä¾èµ–å·²åŠ è½½
    if not load_dependencies():
        return None
    
    model_loading = True
    try:
        model_path = './models/speech_zipenhancer_ans_multiloss_16k_base/onnx_model.onnx'
        if not os.path.exists(model_path):
            raise RuntimeError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        logger.info(f"ğŸ”„ åŠ è½½é™å™ªæ¨¡å‹: {model_path}")
        start_time = time.time()
        
        global_enhancer = StreamingZipEnhancer(
            onnx_model_path=model_path,
            chunk_duration=0.5,  # ğŸ”§ FCä¼˜åŒ–ï¼š0.5ç§’chunk
            overlap_duration=0.1  # å‡å°‘é‡å é™ä½è®¡ç®—é‡
        )
        
        load_time = time.time() - start_time
        logger.info(f"âœ… é™å™ªæ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}s")
        
        return global_enhancer
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        global_enhancer = None
        return None
    finally:
        model_loading = False

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ - FCç¯å¢ƒæ£€æŸ¥"""
    instance_uptime = time.time() - fc_metrics['instance_start_time']
    return {
        "status": "healthy",
        "service": "fc-denoise-service",
        "version": "1.0.0",
        "runtime": "aliyun-fc-3.0",
        "features": ["streaming", "lazy-loading", "fc-optimized"],
        "model_loaded": global_enhancer is not None,
        "dependencies_loaded": dependencies_loaded,
        "ready": True,
        "fc_metrics": {
            "instance_uptime_seconds": f"{instance_uptime:.1f}",
            "cold_starts": fc_metrics['cold_starts'],
            "warm_starts": fc_metrics['warm_starts'],
            "model_reuse_rate": f"{fc_metrics['model_reuse_count']}/{fc_metrics['total_requests']}",
            "performance_optimization": "æ‡’åŠ è½½ + æ¨¡å‹å¤ç”¨ (ç®€åŒ–ç‰ˆ)"
        }
    }

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ - FCæ ‡å‡†"""
    current_time = time.time()
    instance_uptime = current_time - fc_metrics['instance_start_time']
    
    # é¢„æœŸé¦–æ¬¡è¯·æ±‚å»¶è¿Ÿ (ç®€åŒ–ç‰ˆ)
    expected_first_request_latency = "~15ç§’ (æ‡’åŠ è½½æ¨¡å¼)"
    
    return {
        "status": "healthy",
        "service_ready": True,
        "model_loaded": global_enhancer is not None,
        "model_loading": model_loading,
        "dependencies": {
            "torch": dependencies_loaded,
            "onnxruntime": dependencies_loaded,
            "model": global_enhancer is not None
        },
        "fc_optimization": {
            "expected_first_request_latency": expected_first_request_latency,
            "optimization_level": "Lazy Loading (ç®€åŒ–ç‰ˆ)"
        },
        "instance_info": {
            "uptime_seconds": f"{instance_uptime:.1f}",
            "total_requests": fc_metrics['total_requests'],
            "performance_stats": f"Cold: {fc_metrics['cold_starts']}, Warm: {fc_metrics['warm_starts']}"
        },
        "timestamp": current_time,
        "fc_environment": True
    }

@app.post("/")
async def denoise_audio(
    request: Request,
    x_segment_id: Optional[str] = Header(None),
    x_speaker: Optional[str] = Header(None),
    x_enable_streaming: Optional[str] = Header("true"),
    x_input_format: Optional[str] = Header("binary")  # ğŸ†• FC: æ”¯æŒbinary/base64
):
    """å¤„ç†éŸ³é¢‘é™å™ªè¯·æ±‚ - FCé€‚é…ç‰ˆæœ¬"""
    start_time = time.time()
    segment_id = x_segment_id or f"fc_segment_{int(time.time())}"
    speaker = x_speaker or "unknown"
    enable_streaming = x_enable_streaming.lower() == "true"
    input_format = x_input_format or "binary"
    
    # FCç›‘æ§ï¼šè·Ÿè¸ªè¯·æ±‚ç±»å‹
    fc_metrics['total_requests'] += 1
    is_cold_start = global_enhancer is None
    if is_cold_start:
        fc_metrics['cold_starts'] += 1
        logger.info(f"â„ï¸ FCå†·å¯åŠ¨è¯·æ±‚: segment={segment_id}")
    else:
        fc_metrics['warm_starts'] += 1
        fc_metrics['model_reuse_count'] += 1
        logger.info(f"ğŸ”¥ FCçƒ­å¯åŠ¨è¯·æ±‚: segment={segment_id} (æ¨¡å‹å¤ç”¨ç¬¬{fc_metrics['model_reuse_count']}æ¬¡)")
    
    # è·å–éŸ³é¢‘æ•°æ®
    raw_data = await request.body()
    
    logger.info(f"ğŸµ FCé™å™ªè¯·æ±‚: segment={segment_id}, speaker={speaker}, size={len(raw_data)} bytes, format={input_format}")
    
    # ğŸ†• FCé€‚é…ï¼šæ”¯æŒBase64ç¼–ç è¾“å…¥
    try:
        if input_format.lower() == "base64":
            audio_data = base64.b64decode(raw_data)
            logger.info(f"ğŸ“¥ Base64è§£ç å®Œæˆ: {len(raw_data)} -> {len(audio_data)} bytes")
        else:
            audio_data = raw_data
            
    except Exception as e:
        logger.error(f"âŒ æ•°æ®æ ¼å¼è§£æå¤±è´¥: {e}")
        return Response(
            content=f"æ•°æ®æ ¼å¼é”™è¯¯: {e}",
            status_code=400,
            headers={
                "X-Processing-Success": "false",
                "X-Segment-Id": segment_id,
                "X-Error": "data_format_error"
            }
        )
    
    # åˆå§‹åŒ–å˜é‡
    audio = None
    enhanced_audio = None
    
    try:
        # 1. è¯»å–éŸ³é¢‘
        audio, sr = sf.read(io.BytesIO(audio_data))
        logger.info(f"éŸ³é¢‘åŠ è½½æˆåŠŸ: shape={audio.shape}, sr={sr}")
        
        # 2. éªŒè¯é‡‡æ ·ç‡
        if sr != 16000:
            logger.warning(f"âš ï¸ é‡‡æ ·ç‡ä¸åŒ¹é…: {sr}Hz, é¢„æœŸ16kHzï¼Œå°†ç›´æ¥è¿”å›åŸéŸ³é¢‘")
            # ä¸é™å™ªï¼Œç›´æ¥è¿”å›åŸéŸ³é¢‘
            output_buffer = io.BytesIO()
            sf.write(output_buffer, audio, sr, format='WAV', subtype='PCM_16')
            output_buffer.seek(0)
            return Response(
                content=output_buffer.read(),
                media_type="audio/wav",
                headers={
                    "X-Processing-Success": "true",
                    "X-Segment-Id": segment_id,
                    "X-Denoise-Applied": "false",
                    "X-Reason": "sample-rate-mismatch",
                    "X-FC-Environment": "true"
                }
            )
        
        # 3. å°è¯•é™å™ª
        try:
            enhancer = get_enhancer()
            if enhancer is None:
                logger.warning("âš ï¸ é™å™ªæ¨¡å‹æœªåŠ è½½ï¼Œè¿”å›åŸå§‹éŸ³é¢‘")
                enhanced_audio = audio
                denoise_applied = False
            else:
                # æ‰§è¡Œé™å™ª
                if enable_streaming:
                    enhanced_chunks = []
                    for chunk in enhancer.stream_process(audio):
                        enhanced_chunks.append(chunk)
                    enhanced_audio = np.concatenate(enhanced_chunks)
                    logger.info(f"æµå¼é™å™ªå®Œæˆ: {len(enhanced_chunks)} chunks")
                else:
                    enhanced_audio = enhancer.process(audio)
                    logger.info("å•æ¬¡é™å™ªå®Œæˆ")
                denoise_applied = True
                
        except Exception as denoise_error:
            logger.error(f"âŒ é™å™ªå¤„ç†å¤±è´¥: {denoise_error}")
            enhanced_audio = audio
            denoise_applied = False
        
        # 4. ç¡®ä¿è¾“å‡ºèŒƒå›´æ­£ç¡®
        enhanced_audio = np.clip(enhanced_audio, -1.0, 1.0)
        
        # 5. è½¬æ¢å›WAV
        output_buffer = io.BytesIO()
        sf.write(output_buffer, enhanced_audio, sr, format='WAV', subtype='PCM_16')
        output_buffer.seek(0)
        
        # 6. å‡†å¤‡å“åº”
        output_data = output_buffer.read()
        process_time = time.time() - start_time
        
        logger.info(f"âœ… FCå¤„ç†å®Œæˆ: segment={segment_id}, "
                   f"é™å™ª={'æ˜¯' if denoise_applied else 'å¦'}, "
                   f"è€—æ—¶={process_time:.2f}s")
        
        # 7. æ¸…ç†å†…å­˜ - ä½¿ç”¨å¢å¼ºç‰ˆæœ¬
        enhanced_cleanup_memory(audio, enhanced_audio, 
                               force_gc=True,  # FCç¯å¢ƒä¸­å¼ºåˆ¶GC
                               clear_torch_cache=False)  # ä¿ç•™æ¨¡å‹ç¼“å­˜ä¾›åç»­è¯·æ±‚ä½¿ç”¨
        
        return Response(
            content=output_data,
            media_type="audio/wav",
            headers={
                "X-Processing-Success": "true",
                "X-Segment-Id": segment_id,
                "X-Processing-Time": f"{process_time:.3f}",
                "X-Denoise-Applied": str(denoise_applied).lower(),
                "X-Model-Loaded": str(global_enhancer is not None).lower(),
                "X-FC-Environment": "true",
                # FCç‰¹æœ‰ç›‘æ§å¤´
                "X-FC-Instance-Type": "cold" if is_cold_start else "warm",
                "X-FC-Model-Reuse-Count": str(fc_metrics['model_reuse_count']),
                "X-FC-Instance-Uptime": f"{time.time() - fc_metrics['instance_start_time']:.1f}"
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ FCè¯·æ±‚å¤„ç†å¤±è´¥: segment={segment_id}, error={e}")
        
        # å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿæ¸…ç†å†…å­˜
        enhanced_cleanup_memory(audio, enhanced_audio, 
                               force_gc=True,  # å¼‚å¸¸æ—¶å¼ºåˆ¶æ¸…ç†
                               clear_torch_cache=False)  # ä¿ç•™æ¨¡å‹ä¾›åç»­ä½¿ç”¨
        
        return Response(
            content=str(e),
            status_code=500,
            headers={
                "X-Processing-Success": "false",
                "X-Segment-Id": segment_id,
                "X-Error": str(e),
                "X-FC-Environment": "true"
            }
        )

def enhanced_cleanup_memory(*variables, force_gc=True, clear_torch_cache=False):
    """
    å¢å¼ºçš„å†…å­˜æ¸…ç†ï¼Œé€‚é…FCç¯å¢ƒå’Œæ·±åº¦å­¦ä¹ æ¨¡å‹
    
    Args:
        *variables: è¦æ¸…ç†çš„å˜é‡
        force_gc: æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œåƒåœ¾å›æ”¶
        clear_torch_cache: æ˜¯å¦æ¸…ç†Torchç¼“å­˜
    """
    cleanup_start_time = time.time()
    cleanup_count = 0
    
    try:
        # 1. æ¸…ç†æ™®é€šå˜é‡
        for var in variables:
            if var is not None:
                try:
                    # å¦‚æœæ˜¯Torch tensorï¼Œå…ˆdetach
                    if hasattr(var, 'detach'):
                        var = var.detach()
                    # å¦‚æœæ˜¯NumPyæ•°ç»„ï¼Œæ˜¾å¼é‡Šæ”¾
                    elif hasattr(var, 'dtype') and hasattr(var, 'shape'):
                        pass  # NumPyæ•°ç»„ï¼Œç›´æ¥åˆ é™¤
                    del var
                    cleanup_count += 1
                except Exception as e:
                    logger.debug(f"æ¸…ç†å˜é‡å¼‚å¸¸: {e}")
        
        # 2. æ¸…ç†Torchç¼“å­˜ï¼ˆå¯é€‰ï¼‰
        if clear_torch_cache and torch is not None:
            try:
                if hasattr(torch, 'cuda') and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.debug("æ¸…ç†Torch GPUç¼“å­˜")
                # æ¸…ç†CPUç¼“å­˜ï¼ˆå¦‚æœæœ‰ï¼‰
                if hasattr(torch, '_C') and hasattr(torch._C, '_cuda_emptyCache'):
                    pass  # å·²ç»åœ¨ä¸Šé¢å¤„ç†
            except Exception as e:
                logger.debug(f"æ¸…ç†Torchç¼“å­˜å¼‚å¸¸: {e}")
        
        # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼ˆå¯é€‰ï¼‰
        if force_gc:
            collected = gc.collect()
            logger.debug(f"åƒåœ¾å›æ”¶å¯¹è±¡æ•°: {collected}")
        
        cleanup_time = time.time() - cleanup_start_time
        if cleanup_count > 0:
            logger.debug(f"å†…å­˜æ¸…ç†å®Œæˆ: {cleanup_count}ä¸ªå˜é‡, è€—æ—¶: {cleanup_time*1000:.1f}ms")
            
    except Exception as e:
        logger.warning(f"å†…å­˜æ¸…ç†å¼‚å¸¸: {e}")

# ä¿æŒå‘åå…¼å®¹ï¼ŒåŒæ—¶ä½¿ç”¨å¢å¼ºç‰ˆæœ¬
def cleanup_memory(*variables):
    """å‘åå…¼å®¹çš„å†…å­˜æ¸…ç†å‡½æ•°"""
    enhanced_cleanup_memory(*variables, force_gc=True, clear_torch_cache=False)

# FCç¯å¢ƒå¯åŠ¨æ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼Œæ— é¢„çƒ­ï¼‰
@app.on_event("startup")
async def startup_event():
    """FCç¯å¢ƒå¯åŠ¨æ£€æŸ¥ - ç®€åŒ–ç‰ˆï¼Œé¿å…é¢„çƒ­è¶…æ—¶"""
    logger.info("ğŸš€ FCé™å™ªæœåŠ¡å¯åŠ¨")
    logger.info(f"ğŸ“Š ç¯å¢ƒä¿¡æ¯:")
    logger.info(f"  - è¿è¡Œæ—¶: é˜¿é‡Œäº‘å‡½æ•°è®¡ç®— 3.0")
    logger.info(f"  - ç›‘å¬ç«¯å£: 9000")
    logger.info(f"  - æ¨¡å‹è·¯å¾„: ./models/speech_zipenhancer_ans_multiloss_16k_base/")
    logger.info(f"  - ç‰¹æ€§: æ‡’åŠ è½½ + æµå¼å¤„ç† + FCä¼˜åŒ–")
    logger.info(f"  - é¦–æ¬¡è¯·æ±‚é¢„æœŸå»¶è¿Ÿ: ~15ç§’ (æ‡’åŠ è½½æ¨¡å¼)")
    
    # åŸºç¡€ç›‘æ§æŒ‡æ ‡åˆå§‹åŒ– (ç®€åŒ–ç‰ˆ)
    logger.info("âœ… FCé™å™ªæœåŠ¡å°±ç»ªï¼Œç­‰å¾…é¦–æ¬¡è¯·æ±‚")

# ğŸ—‘ï¸ FC PreStopå›è°ƒ - å®ä¾‹é”€æ¯å‰æ¸…ç†
@app.on_event("shutdown")
async def fc_prestop():
    """FC PreStopå›è°ƒ - å®ä¾‹é”€æ¯å‰æ¸…ç†ï¼Œç¡®ä¿èµ„æºæ­£å¸¸é‡Šæ”¾"""
    logger.info("ğŸ—‘ï¸ FCå®ä¾‹å³å°†é”€æ¯ï¼Œæ‰§è¡Œèµ„æºæ¸…ç†...")
    
    prestop_start_time = time.time()
    instance_lifetime = prestop_start_time - fc_metrics['instance_start_time']
    
    # è®°å½•å®ä¾‹ç”Ÿå‘½å‘¨æœŸç»Ÿè®¡
    logger.info(f"ğŸ“Š å®ä¾‹ç”Ÿå‘½å‘¨æœŸç»Ÿè®¡:")
    logger.info(f"  - æ€»è¿è¡Œæ—¶é—´: {instance_lifetime:.1f}ç§’")
    logger.info(f"  - å¤„ç†è¯·æ±‚æ•°: {fc_metrics['total_requests']}ä¸ª")
    logger.info(f"  - å†·å¯åŠ¨/çƒ­å¯åŠ¨: {fc_metrics['cold_starts']}/{fc_metrics['warm_starts']}")
    logger.info(f"  - æ¨¡å‹å¤ç”¨æ¬¡æ•°: {fc_metrics['model_reuse_count']}æ¬¡")
    
    # æ¸…ç†å…¨å±€èµ„æº
    global global_enhancer, torch, onnxruntime
    cleanup_count = 0
    
    try:
        if global_enhancer:
            logger.info("ğŸ—‘ï¸ æ¸…ç†æ¨¡å‹å®ä¾‹...")
            del global_enhancer
            global_enhancer = None
            cleanup_count += 1
            
        # æ¸…ç†torchç¼“å­˜
        if torch is not None:
            try:
                if hasattr(torch, 'cuda') and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("ğŸ—‘ï¸ æ¸…ç†Torch GPUç¼“å­˜")
            except:
                pass
                
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        collected = gc.collect()
        logger.info(f"ğŸ—‘ï¸ åƒåœ¾å›æ”¶å¯¹è±¡æ•°: {collected}")
        
        prestop_time = time.time() - prestop_start_time
        logger.info(f"âœ… FCèµ„æºæ¸…ç†å®Œæˆï¼Œè€—æ—¶: {prestop_time:.2f}s")
        
    except Exception as e:
        logger.error(f"âŒ FCèµ„æºæ¸…ç†å¼‚å¸¸: {e}")
    
    logger.info("ğŸ‘‹ FCå®ä¾‹æ­£å¸¸å…³é—­")

if __name__ == "__main__":
    import uvicorn
    print("[FC-STARTUP] Starting FC denoise server...")
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=9000,  # âœ… FCå¥åº·æ£€æŸ¥ç«¯å£
        log_level="info"
    )