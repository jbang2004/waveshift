#!/usr/bin/env python3
"""
ZipEnhancer æµå¼éŸ³é¢‘é™å™ªå·¥å…·
æ¼”ç¤ºå¦‚ä½•å°†éŸ³é¢‘åˆ†å—å¤„ç†ä»¥èŠ‚çœå†…å­˜
"""

import soundfile as sf
import numpy as np
import torch
import onnxruntime
import librosa
from typing import Generator, Tuple

def mag_pha_stft(signal, n_fft=400, hop_size=100, win_size=400, compress_factor=0.3):
    """STFTå˜æ¢"""
    window = torch.hann_window(win_size)
    stft = torch.stft(signal, n_fft, hop_size, win_size, window, center=True, 
                      pad_mode='reflect', return_complex=True)
    magnitude = torch.pow(torch.abs(stft), compress_factor)
    phase = torch.angle(stft)
    return magnitude, phase

def mag_pha_istft(magnitude_compressed, phase, n_fft=400, hop_size=100, win_size=400, compress_factor=0.3):
    """ISTFTå˜æ¢"""
    magnitude = torch.pow(magnitude_compressed, 1.0 / compress_factor)
    real = magnitude * torch.cos(phase)
    imag = magnitude * torch.sin(phase)
    stft_complex = torch.complex(real, imag)
    window = torch.hann_window(win_size)
    signal = torch.istft(stft_complex, n_fft, hop_size, win_size, window, center=True)
    return signal

class StreamingZipEnhancer:
    """æµå¼ZipEnhancerå¤„ç†å™¨"""
    
    def __init__(self, onnx_model_path: str, chunk_size: int = 16000, overlap_size: int = 1600, 
                 chunk_duration: float = 1.0, overlap_duration: float = 0.5):
        """
        åˆå§‹åŒ–æµå¼å¤„ç†å™¨
        
        Args:
            onnx_model_path: ONNXæ¨¡å‹è·¯å¾„
            chunk_size: æ¯ä¸ªå¤„ç†å—çš„å¤§å°ï¼ˆæ ·æœ¬æ•°ï¼‰
            overlap_size: é‡å åŒºåŸŸå¤§å°ï¼ˆç”¨äºæ¶ˆé™¤è¾¹ç•Œæ•ˆåº”ï¼‰
            chunk_duration: æ¯ä¸ªå¤„ç†å—çš„æ—¶é•¿ï¼ˆç§’ï¼‰
            overlap_duration: å—ä¹‹é—´çš„é‡å æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        self.session = onnxruntime.InferenceSession(onnx_model_path)
        self.chunk_size = chunk_size
        self.overlap_size = overlap_size
        self.previous_chunk = None
        
        # æ”¯æŒæ—¶é—´å‚æ•°
        self.sample_rate = 16000
        self.chunk_duration = chunk_duration
        self.overlap_duration = overlap_duration
        
        # STFTå‚æ•°
        self.n_fft = 400
        self.hop_size = 100
        self.win_size = 400
        self.compress_factor = 0.3
        
    def process_chunk(self, chunk: np.ndarray) -> np.ndarray:
        """å¤„ç†å•ä¸ªéŸ³é¢‘å—"""
        # è½¬æ¢ä¸ºtorchå¼ é‡
        chunk_tensor = torch.from_numpy(chunk.astype(np.float32)).unsqueeze(0)
        
        # RMSå½’ä¸€åŒ–
        rms = torch.sqrt(torch.mean(chunk_tensor ** 2))
        norm_factor = 1.0 / (rms + 1e-10)
        normalized_chunk = chunk_tensor * norm_factor
        
        # STFT
        chunk_amp, chunk_pha = mag_pha_stft(normalized_chunk)
        
        # ONNXæ¨ç†
        ort_inputs = {
            self.session.get_inputs()[0].name: chunk_amp.numpy(),
            self.session.get_inputs()[1].name: chunk_pha.numpy()
        }
        ort_outs = self.session.run(None, ort_inputs)
        
        # ISTFTé‡æ„
        amp_g = torch.from_numpy(ort_outs[0])
        pha_g = torch.from_numpy(ort_outs[1])
        enhanced_chunk = mag_pha_istft(amp_g, pha_g)
        
        # åå½’ä¸€åŒ–
        enhanced_chunk = enhanced_chunk / norm_factor
        
        return enhanced_chunk.numpy()[0]
    
    def stream_process(self, audio: np.ndarray) -> Generator[np.ndarray, None, None]:
        """
        æµå¼å¤„ç†éŸ³é¢‘
        
        Args:
            audio: è¾“å…¥éŸ³é¢‘æ•°ç»„
            
        Yields:
            å¤„ç†åçš„éŸ³é¢‘å—
        """
        total_samples = len(audio)
        processed_samples = 0
        
        while processed_samples < total_samples:
            # è®¡ç®—å½“å‰å—çš„èµ·å§‹å’Œç»“æŸä½ç½®
            start = processed_samples
            end = min(start + self.chunk_size, total_samples)
            
            # æå–å½“å‰å—
            current_chunk = audio[start:end]
            
            # å¦‚æœä¸æ˜¯ç¬¬ä¸€å—ï¼Œæ·»åŠ é‡å åŒºåŸŸ
            if self.previous_chunk is not None:
                # ä»å‰ä¸€å—å–é‡å éƒ¨åˆ†
                overlap_part = self.previous_chunk[-self.overlap_size:]
                current_chunk = np.concatenate([overlap_part, current_chunk])
            
            # å¤„ç†å½“å‰å—
            enhanced_chunk = self.process_chunk(current_chunk)
            
            # å¦‚æœæœ‰é‡å ï¼Œè·³è¿‡é‡å éƒ¨åˆ†
            if self.previous_chunk is not None:
                enhanced_chunk = enhanced_chunk[self.overlap_size:]
            
            # ä¿å­˜å½“å‰å—ä¾›ä¸‹æ¬¡ä½¿ç”¨
            self.previous_chunk = current_chunk
            
            # è¾“å‡ºå¤„ç†ç»“æœ
            yield enhanced_chunk
            
            processed_samples = end
    
    def process(self, audio: np.ndarray) -> np.ndarray:
        """
        ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªéŸ³é¢‘ï¼ˆé€‚åˆçŸ­éŸ³é¢‘ï¼‰
        
        Args:
            audio: è¾“å…¥éŸ³é¢‘æ•°ç»„
            
        Returns:
            é™å™ªåçš„éŸ³é¢‘æ•°ç»„
        """
        # å¯¹äºçŸ­éŸ³é¢‘ï¼Œç›´æ¥å¤„ç†æ•´ä¸ªéŸ³é¢‘
        if len(audio) <= self.chunk_size * 2:
            return self.process_chunk(audio)
        
        # å¯¹äºé•¿éŸ³é¢‘ï¼Œä½¿ç”¨æµå¼å¤„ç†å¹¶åˆå¹¶ç»“æœ
        enhanced_chunks = []
        for chunk in self.stream_process(audio):
            enhanced_chunks.append(chunk)
        
        return np.concatenate(enhanced_chunks)

def streaming_denoise(input_path: str, output_path: str, 
                     onnx_model_path: str = './speech_zipenhancer_ans_multiloss_16k_base/onnx_model.onnx',
                     chunk_duration: float = 1.0):
    """
    æµå¼éŸ³é¢‘é™å™ª
    
    Args:
        input_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
        output_path: è¾“å‡ºéŸ³é¢‘è·¯å¾„  
        onnx_model_path: ONNXæ¨¡å‹è·¯å¾„
        chunk_duration: æ¯å—å¤„ç†æ—¶é•¿ï¼ˆç§’ï¼‰
    """
    print(f"ğŸŒŠ å¼€å§‹æµå¼é™å™ªå¤„ç†: {input_path}")
    
    # è¯»å–éŸ³é¢‘ï¼ˆå¯ä»¥è€ƒè™‘ä½¿ç”¨librosaçš„æµå¼è¯»å–ï¼‰
    audio, sr = librosa.load(input_path, sr=16000, mono=True)
    
    # è®¡ç®—å—å¤§å°
    chunk_size = int(chunk_duration * sr)
    overlap_size = chunk_size // 10  # 10%é‡å 
    
    print(f"ğŸ“Š éŸ³é¢‘ä¿¡æ¯: {sr}Hz, {len(audio)/sr:.2f}ç§’")
    print(f"ğŸ“Š å¤„ç†é…ç½®: å—å¤§å°={chunk_size}æ ·æœ¬, é‡å ={overlap_size}æ ·æœ¬")
    
    # åˆ›å»ºæµå¼å¤„ç†å™¨
    enhancer = StreamingZipEnhancer(onnx_model_path, chunk_size, overlap_size)
    
    # æ”¶é›†æ‰€æœ‰å¤„ç†åçš„å—
    enhanced_chunks = []
    
    for enhanced_chunk in enhancer.stream_process(audio):
        enhanced_chunks.append(enhanced_chunk)
    
    # åˆå¹¶æ‰€æœ‰å—
    enhanced_audio = np.concatenate(enhanced_chunks)
    
    # ç¡®ä¿é•¿åº¦åŒ¹é…ï¼ˆå¯èƒ½å› ä¸ºé‡å å¤„ç†æœ‰å°å¹…å·®å¼‚ï¼‰
    if len(enhanced_audio) > len(audio):
        enhanced_audio = enhanced_audio[:len(audio)]
    elif len(enhanced_audio) < len(audio):
        # ç”¨é›¶å¡«å……
        padding = np.zeros(len(audio) - len(enhanced_audio))
        enhanced_audio = np.concatenate([enhanced_audio, padding])
    
    # ä¿å­˜ç»“æœ
    enhanced_audio = np.clip(enhanced_audio, -1.0, 1.0)
    sf.write(output_path, (enhanced_audio * 32767).astype(np.int16), sr)
    
    print(f"âœ… æµå¼é™å™ªå®Œæˆ! è¾“å‡º: {output_path}")
    print(f"ğŸ“ˆ å†…å­˜å³°å€¼çº¦ä¸º: {chunk_size * 4 / 1024 / 1024:.1f}MB (vs æ‰¹å¤„ç†: {len(audio) * 4 / 1024 / 1024:.1f}MB)")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        input_file = "./speech_zipenhancer_ans_multiloss_16k_base/examples/noisy_sample.wav"
        output_file = "streaming_enhanced.wav"
    else:
        input_file = sys.argv[1]
        output_file = sys.argv[2] if len(sys.argv) > 2 else "streaming_enhanced.wav"
    
    streaming_denoise(input_file, output_file) 