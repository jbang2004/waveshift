import { WorkflowEntrypoint, WorkflowStep, WorkflowEvent } from 'cloudflare:workers';
import { Env, SepTransWorkflowParams } from './types/env.d';
import { updateMediaTaskStatus, updateMediaTaskUrls, createTranscription, completeMediaTask, setMediaTaskError, updateTranscriptionTotalSegments, markLastTranscriptionSegment } from './utils/database';
import { createMediaUrlManager } from './utils/url-utils';
import { 
  initRealtimeMergeState, 
  processSegmentRealtime, 
  storeSegmentToD1, 
  TranscriptionSegment,
  RealtimeMergeState 
} from './utils/transcription-merger';

export class SepTransWorkflow extends WorkflowEntrypoint<Env, SepTransWorkflowParams> {
	async run(event: WorkflowEvent<SepTransWorkflowParams>, step: WorkflowStep) {
		const env = this.env;
		const { originalFile, fileType, options } = event.payload;
		const taskId = event.instanceId;
		const startTime = Date.now();
		
		console.log(`å¼€å§‹ SepTransWorkflow ä»»åŠ¡: ${taskId}, æ–‡ä»¶ç±»å‹: ${fileType}`);
		
		try {
			// æ­¥éª¤0: æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºåˆ†ç¦»ä¸­
			await step.do("update-status-separating", async () => {
				console.log(`æ­¥éª¤0: æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºåˆ†ç¦»ä¸­ ${taskId}`);
				await updateMediaTaskStatus(env, taskId, 'separating', 10);
			});
			
			// æ­¥éª¤1: éŸ³è§†é¢‘åˆ†ç¦» (å§”æ‰˜ç»™ ffmpeg-worker)
			const { audioUrl, videoUrl, audioKey } = await step.do("separate-media", async () => {
				console.log(`æ­¥éª¤1: å¼€å§‹éŸ³è§†é¢‘åˆ†ç¦» ${taskId}`);
				
				// ä»åŸå§‹æ–‡ä»¶è·¯å¾„ä¸­æå–userIdï¼ˆæ ¼å¼: users/{userId}/{taskId}/original.{ext}ï¼‰
				const pathParts = originalFile.split('/');
				const userId = pathParts[1];
				
				// å®šä¹‰è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆä¿å­˜åœ¨åŒä¸€ç›®å½•ä¸‹ï¼‰
				const audioOutputKey = `users/${userId}/${taskId}/audio.aac`;
				const videoOutputKey = `users/${userId}/${taskId}/video.mp4`;
				
				// è°ƒç”¨ ffmpeg-worker (Service Binding)
				const result = await env.FFMPEG_SERVICE.separate({
					inputKey: originalFile,
					audioOutputKey,
					videoOutputKey
				});
				
				// ä½¿ç”¨ç»Ÿä¸€çš„URLç®¡ç†å™¨ç”Ÿæˆå…¬å…±URL
				const urlManager = createMediaUrlManager(env.R2_PUBLIC_DOMAIN);
				const audioUrl = urlManager.buildPublicUrl(audioOutputKey);
				const videoUrl = urlManager.buildPublicUrl(videoOutputKey);
				
				// æ›´æ–°æ•°æ®åº“ (å­˜å‚¨ç›¸å¯¹è·¯å¾„è€Œéå®Œæ•´URL)
				await updateMediaTaskUrls(env, taskId, audioOutputKey, videoOutputKey);
				await updateMediaTaskStatus(env, taskId, 'transcribing', 40);
				
				console.log(`éŸ³è§†é¢‘åˆ†ç¦»å®Œæˆ - è§†é¢‘: ${videoUrl}, éŸ³é¢‘: ${audioUrl}`);
				console.log(`æ–‡ä»¶å¤§å° - è§†é¢‘: ${result.videoSize} bytes, éŸ³é¢‘: ${result.audioSize} bytes`);
				
				return { 
					audioKey: audioOutputKey, 
					videoKey: videoOutputKey,
					audioUrl,
					videoUrl
				};
			});
			
			// æ­¥éª¤2: åˆ›å»ºè½¬å½•è®°å½•ï¼ˆä¸ºå¹¶è¡Œå¤„ç†å‡†å¤‡ï¼‰
			const transcriptionId = await step.do("create-transcription", async () => {
				const id = await createTranscription(
					env,
					taskId,
					options.targetLanguage,
					options.style || 'normal',
					{ startTime: new Date().toISOString() }
				);
				console.log(`ğŸ“ åˆ›å»ºè½¬å½•è®°å½•: ${id}`);
				return id;
			});
			
			// ğŸ”¥ æ­¥éª¤3: å¹¶è¡Œå¤„ç† - åŒæ—¶å¯åŠ¨è½¬å½•å’ŒéŸ³é¢‘åˆ‡åˆ†
			const parallelStartTime = Date.now();
			console.log(`ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†: è½¬å½• + éŸ³é¢‘åˆ‡åˆ†`);
			
			const [transcriptionResult, audioSegmentResult] = await Promise.all([
				// 3a. è½¬å½•æœåŠ¡ï¼ˆæµå¼å†™å…¥D1ï¼‰
				step.do("realtime-transcribe", async () => {
					console.log(`ğŸ™ï¸ å¯åŠ¨è½¬å½•æœåŠ¡...`);
					return await this.runTranscription(env, {
						audioKey,
						transcriptionId,
						options,
						taskId
					});
				}),
				
				// 3b. éŸ³é¢‘åˆ‡åˆ†æœåŠ¡ï¼ˆè½®è¯¢D1ï¼‰
				step.do("watch-audio-segment", async () => {
					// ç»™è½¬å½•æœåŠ¡ä¸€ç‚¹å¯åŠ¨æ—¶é—´ï¼Œé¿å…ç©ºè½®è¯¢
					console.log(`â³ ç­‰å¾…3ç§’åå¯åŠ¨éŸ³é¢‘åˆ‡åˆ†...`);
					await new Promise(resolve => setTimeout(resolve, 3000));
					
					console.log(`âœ‚ï¸ å¯åŠ¨éŸ³é¢‘åˆ‡åˆ†æœåŠ¡...`);
					const pathParts = originalFile.split('/');
					const userId = pathParts[1];
					
					// ğŸ”§ ä¿®å¤ï¼šç¡®ä¿outputPrefixæ ¼å¼æ­£ç¡®ï¼Œå¢åŠ å®¹é”™å¤„ç†
					if (!userId || userId.trim() === '') {
						throw new Error(`æ— æ³•ä»originalFileè·¯å¾„ä¸­æå–userId: ${originalFile}`);
					}
					
					const outputPrefix = `users/${userId}/${taskId}/audio-segments`;
					console.log(`ğŸ“ éŸ³é¢‘åˆ‡åˆ†è¾“å‡ºè·¯å¾„: ${outputPrefix}`);
					
					return await env.AUDIO_SEGMENT_SERVICE.watch({
						audioKey,
						transcriptionId,
						outputPrefix,
						taskId,
						enableDenoising: options.enableDenoising || false  // ğŸ†• ä¼ é€’é™å™ªé€‰é¡¹
					});
				})
			]);
			
			const parallelDuration = Date.now() - parallelStartTime;
			console.log(`âœ… å¹¶è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶: ${parallelDuration}ms`);
			
			// æ­¥éª¤4: å®Œæˆå¤„ç†
			await step.do("finalize", async () => {
				await completeMediaTask(env, taskId, true);
				const totalDuration = Date.now() - startTime;
				console.log(`ğŸ“Š å¤„ç†ç»Ÿè®¡:`);
				console.log(`  - æ€»è€—æ—¶: ${totalDuration}ms`);
				console.log(`  - å¹¶è¡Œå¤„ç†è€—æ—¶: ${parallelDuration}ms`);
				console.log(`  - è½¬å½•ç‰‡æ®µæ•°: ${transcriptionResult.totalSegments}`);
				console.log(`  - éŸ³é¢‘åˆ‡ç‰‡æ•°: ${audioSegmentResult.segmentCount}`);
				console.log(`  - è§†é¢‘URL: ${videoUrl}`);
				console.log(`  - éŸ³é¢‘URL: ${audioUrl}`);
			});
			
		} catch (error: any) {
			console.error(`SepTransWorkflow å¤±è´¥: ${taskId}`, error);
			
			// æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
			await step.do("mark-failed", async () => {
				await setMediaTaskError(env, taskId, error.message, error.stack);
			});
			
			// è®°å½•é”™è¯¯ä¿¡æ¯
			await step.do("log-error", async () => {
				console.error(`âŒ å·¥ä½œæµå¤±è´¥è¯¦æƒ… ${taskId}:`, {
					message: error.message,
					stack: error.stack?.substring(0, 500)
				});
			});
			
			throw error;
		}
	}
	
	// è½¬å½•å¤„ç†æ–¹æ³•ï¼ˆä¿æŒç°æœ‰é€»è¾‘ï¼‰
	private async runTranscription(env: Env, params: {
		audioKey: string;
		transcriptionId: string;
		options: any;
		taskId: string;
	}) {
		const { audioKey, transcriptionId, options, taskId } = params;
		
		// ä½¿ç”¨ç°æœ‰çš„å®æ—¶è½¬å½•é€»è¾‘
		const mergeState: RealtimeMergeState = initRealtimeMergeState(
			transcriptionId, 
			options.targetLanguage
		);
		
		console.log(`ğŸ™ï¸ å¼€å§‹å®æ—¶éŸ³é¢‘è½¬å½•ï¼Œè¯­è¨€: ${options.targetLanguage}`);
		
		// è·å–éŸ³é¢‘æ•°æ®å¹¶å‘é€è½¬å½•è¯·æ±‚
		const audioObject = await env.MEDIA_STORAGE.get(audioKey);
		if (!audioObject) {
			throw new Error(`R2 éŸ³é¢‘å¯¹è±¡æœªæ‰¾åˆ°: ${audioKey}`);
		}
		const audioData = await audioObject.arrayBuffer();
		console.log(`éŸ³é¢‘è¯»å–å®Œæˆï¼Œå¤§å°: ${audioData.byteLength} bytes`);
		
		// å‘é€è½¬å½•è¯·æ±‚
		const formData = new FormData();
		formData.append('file', new Blob([audioData], { type: 'audio/aac' }), 'audio.aac');
		formData.append('targetLanguage', options.targetLanguage);
		formData.append('style', options.style || 'normal');

		const result = await env.TRANSCRIBE_SERVICE.fetch(new Request('https://transcribe/transcribe', {
			method: 'POST',
			body: formData
		}));

		if (!result.ok) {
			throw new Error(`è½¬å½•æœåŠ¡è°ƒç”¨å¤±è´¥: ${result.status}`);
		}

		// å®æ—¶å¤„ç†SSEæµ
		const reader = result.body?.getReader();
		if (!reader) {
			throw new Error('æ— æ³•è¯»å–è½¬å½•æœåŠ¡å“åº”æµ');
		}

		const decoder = new TextDecoder();
		let metadata: any = null;

		// æ—¶é—´æ ¼å¼è§£æå‡½æ•°: "XmYsZms" -> æ¯«ç§’æ•°
		const parseTimeToMs = (timeStr: string): number => {
			const match = timeStr.match(/(\d+)m(\d+)s(\d+)ms/);
			if (!match) return 0;
			const [, minutes, seconds, ms] = match;
			return parseInt(minutes) * 60000 + parseInt(seconds) * 1000 + parseInt(ms);
		};

		try {
			while (true) {
				const { done, value } = await reader.read();
				if (done) break;

				const chunk = decoder.decode(value, { stream: true });
				const lines = chunk.split('\n');

				for (const line of lines) {
					if (line.startsWith('data: ')) {
						try {
							const data = JSON.parse(line.slice(6));

							if (data.type === 'start') {
								metadata = data.metadata;
								console.log(`ğŸ¯ å¼€å§‹å®æ—¶è½¬å½•å¤„ç†: ${metadata.fileName}`);
							} else if (data.type === 'segment') {
								// æ„é€ è½¬å½•ç‰‡æ®µ
								const segment: TranscriptionSegment = {
									sequence: data.segment.sequence,
									start_ms: parseTimeToMs(data.segment.start || '0m0s0ms'),
									end_ms: parseTimeToMs(data.segment.end || '0m0s0ms'),
									content_type: data.segment.content_type || 'speech',
									speaker: data.segment.speaker || 'unknown',
									original: data.segment.original || '',
									translation: data.segment.translation || ''
								};
								
								// æ ¸å¿ƒæ”¹è¿›ï¼šå®æ—¶å¤„ç†æ¯ä¸ªç‰‡æ®µ
								const storedSegment = await processSegmentRealtime(env, mergeState, segment);
								
								// ä»…ç”¨äºæ—¥å¿—è®°å½•
								if (storedSegment) {
									console.log(`ğŸ’¾ å­˜å‚¨ç‰‡æ®µ: sequence=${storedSegment.sequence}, speaker=${storedSegment.speaker}`);
								}
							} else if (data.type === 'error') {
								throw new Error(`è½¬å½•æœåŠ¡é”™è¯¯: ${data.error}`);
							}
						} catch (parseError) {
							// å¿½ç•¥è§£æé”™è¯¯çš„è¡Œ
						}
					}
				}
			}
		} finally {
			reader.releaseLock();
		}

		// å¤„ç†æœ€åä¸€ä¸ªå¾…åˆå¹¶çš„ç»„
		if (mergeState.currentGroup) {
			const isFirst = !mergeState.isFirstSegmentStored;
			await storeSegmentToD1(env, mergeState.transcriptionId, mergeState.currentGroup, ++mergeState.lastStoredSequence, isFirst);
			console.log(`ğŸ’¾ å­˜å‚¨æœ€åä¸€ä¸ªåˆå¹¶ç»„: sequence=${mergeState.lastStoredSequence}, is_first=${isFirst}`);
		}

		// æ ‡è®°æœ€åä¸€ä¸ªç‰‡æ®µä¸º is_last=true
		if (mergeState.lastStoredSequence > 0) {
			await markLastTranscriptionSegment(env, transcriptionId);
			console.log(`ğŸ æ ‡è®°æœ€åç‰‡æ®µå®Œæˆ: transcription_id=${transcriptionId}`);
		}

		// æ›´æ–°è½¬å½•è®°å½•çš„æ€»ç‰‡æ®µæ•°
		await updateTranscriptionTotalSegments(env, transcriptionId, mergeState.lastStoredSequence);
		
		// æ›´æ–°ä»»åŠ¡çŠ¶æ€
		await updateMediaTaskStatus(env, taskId, 'completed', 90);
		
		console.log(`âœ… å®æ—¶è½¬å½•å®Œæˆ: ID=${transcriptionId}, æœ€ç»ˆç‰‡æ®µæ•°=${mergeState.lastStoredSequence}`);
		
		return {
			transcriptionId,
			totalSegments: mergeState.lastStoredSequence,
			metadata
		};
	}
}